{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3048.5734185694337\n",
      "[3146.113018576096, 2429.0060682344188, 3290.4553184622655, 3029.8644297774235, 3485.3023413986452, 2810.9853548332276, 3667.672873398836, 2892.8727504456824, 3635.888559663723, 2097.573470904018]\n",
      "[353015.33186975 175053.63124826 308601.39216072 ... 382027.90978527\n",
      " 501641.74887146 477589.36363759]\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "n = 1729\n",
    "randomForest = list()\n",
    "\n",
    "data1 = np.array(train_data)\n",
    "data2 = np.array(test_data)\n",
    "\n",
    "\n",
    "k = 1\n",
    "while (k<=10):\n",
    "    if k == 1:\n",
    "        a = n * k\n",
    "        predictionsTest0 = data1[:a, 1] / 100000\n",
    "        dataTest0 = data1[:a, 2:]\n",
    "        predictionsTrain0 = data1[a:, 1] / 100000\n",
    "        dataTrain0 = data1[a:, 2:]\n",
    "    elif (k>1) & (k<10):\n",
    "        a = n * (k-1)\n",
    "        b = n * k\n",
    "        predictionsTest0 = data1[a:b, 1] / 100000\n",
    "        dataTest0 = data1[a:b, 2:]\n",
    "        predictions_a = data1[0:a, 1] / 100000\n",
    "        data_a = data1[0:a, 2:]\n",
    "        predictions_b = data1[b:, 1] / 100000\n",
    "        data_b = data1[b:, 2:]\n",
    "        predictionsTrain0 = np.concatenate([predictions_a, predictions_b])\n",
    "        dataTrain0 = np.concatenate([data_a, data_b])\n",
    "    else:\n",
    "        a = n * (k-1)\n",
    "        predictionsTrain0 = data1[:a, 1] / 100000\n",
    "        dataTrain0 = data1[:a, 2:]\n",
    "        predictionsTest0 = data1[a:, 1] / 100000\n",
    "        dataTest0 = data1[a:, 2:]\n",
    "    \n",
    "    regression = RandomForestRegressor(max_depth = 15, n_estimators = 250, max_features=0.75, min_samples_split=25)\n",
    "    regression.fit(dataTrain0, predictionsTrain0)\n",
    "    y_0 = regression.predict(dataTest0)\n",
    "    terrorList =predictionsTest0 - y_0\n",
    "    terror = sum(terrorList*terrorList)\n",
    "    randomForest.append(terror)\n",
    "    #print (terror)\n",
    "    \n",
    "    k = k+1\n",
    "    #print (k-1)\n",
    "    #print (test1.shape)\n",
    "    #print (test2.shape)\n",
    "\n",
    "#test1 = data1[0:1729,]\n",
    "#test2 = data1[1728:3457,]\n",
    "#test = test1.to_frame().join(test2.to_frame())\n",
    "#test = np.concatenate([test1, test2])\n",
    "#print (test.shape)\n",
    "n = sum(randomForest)/10\n",
    "print (n)\n",
    "print (randomForest)\n",
    "\n",
    "predictionsTrain = data1[:, 1] / 100000\n",
    "dataTrain = data1[:, 2:]\n",
    "\n",
    "dataTest = data2[:, 1:]\n",
    "        \n",
    "regr_1 = RandomForestRegressor(max_depth = 15, n_estimators = 250, max_features=0.75, min_samples_split=25)\n",
    "regr_1.fit(dataTrain, predictionsTrain)\n",
    "\n",
    "y_1 = regr_1.predict(dataTest) * 100000\n",
    "print (y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2734.058493281943\n",
      "[3335.016371247033, 2277.144567741761, 2792.2843820130806, 2787.9578891040483, 2861.180838374558, 2860.5853948821314, 2663.743404249764, 2456.684301666629, 3244.933055597079, 2061.054727943354]\n",
      "[304036.10621358 208085.15515881 281577.30089667 ... 404026.62049872\n",
      " 491848.44884011 523227.62527803]\n"
     ]
    }
   ],
   "source": [
    "# Deep learning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "data1 = np.array(train_data)\n",
    "data2 = np.array(test_data)\n",
    "\n",
    "n = 1729\n",
    "deepLearning = list()\n",
    "\n",
    "k = 1\n",
    "while (k<=10):\n",
    "    if k == 1:\n",
    "        a = n * k\n",
    "        predictionsTest0 = data1[:a, 1] / 100000\n",
    "        dataTest0 = data1[:a, 2:]\n",
    "        predictionsTrain0 = data1[a:, 1] / 100000\n",
    "        dataTrain0 = data1[a:, 2:]\n",
    "    elif (k>1) & (k<10):\n",
    "        a = n * (k-1)\n",
    "        b = n * k\n",
    "        predictionsTest0 = data1[a:b, 1] / 100000\n",
    "        dataTest0 = data1[a:b, 2:]\n",
    "        predictions_a = data1[0:a, 1] / 100000\n",
    "        data_a = data1[0:a, 2:]\n",
    "        predictions_b = data1[b:, 1] / 100000\n",
    "        data_b = data1[b:, 2:]\n",
    "        predictionsTrain0 = np.concatenate([predictions_a, predictions_b])\n",
    "        dataTrain0 = np.concatenate([data_a, data_b])\n",
    "    else:\n",
    "        a = n * (k-1)\n",
    "        predictionsTrain0 = data1[:a, 1] / 100000\n",
    "        dataTrain0 = data1[:a, 2:]\n",
    "        predictionsTest0 = data1[a:, 1] / 100000\n",
    "        dataTest0 = data1[a:, 2:]\n",
    "    \n",
    "    tmean0 = np.mean(dataTrain0, axis=0)\n",
    "    tstd0  = np.std(dataTrain0, axis=0, dtype=np.float64)\n",
    "    \n",
    "    dataTrain0 = (dataTrain0 - tmean0) / tstd0\n",
    "    dataTest0  = (dataTest0  - tmean0) / tstd0\n",
    "    \n",
    "    regression = MLPRegressor(hidden_layer_sizes=(20, 15, 20), batch_size = 100, max_iter = 500, verbose = False, validation_fraction = 0.1, alpha = 0.0005, learning_rate_init = 0.0005)\n",
    "    regression.fit(dataTrain0, predictionsTrain0)\n",
    "    y_0 = regression.predict(dataTest0)\n",
    "    terrorList =predictionsTest0 - y_0\n",
    "    terror = sum(terrorList*terrorList)\n",
    "    deepLearning.append(terror)\n",
    "    \n",
    "    k = k+1\n",
    "    \n",
    "n = sum(deepLearning)/10\n",
    "print (n)\n",
    "print (deepLearning)\n",
    "\n",
    "\n",
    "predictionsTrain = data1[:, 1] / 100000\n",
    "dataTrain = data1[:, 2:]\n",
    "\n",
    "tmean = np.mean(dataTrain, axis=0)\n",
    "tstd  = np.std(dataTrain, axis=0, dtype=np.float64)\n",
    "\n",
    "dataTest = data2[:, 1:]\n",
    "\n",
    "dataTrain = (dataTrain - tmean) / tstd\n",
    "dataTest  = (dataTest  - tmean) / tstd\n",
    "\n",
    "regr_1 = MLPRegressor(hidden_layer_sizes=(20, 15, 20), batch_size = 100, max_iter = 500, verbose = False, validation_fraction = 0.1, alpha = 0.0005, learning_rate_init = 0.0005)\n",
    "regr_1.fit(dataTrain, predictionsTrain)\n",
    "\n",
    "y_1 = regr_1.predict(dataTest) * 100000\n",
    "print (y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
