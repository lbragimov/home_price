{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempt 0\n",
      "attempt 0, average error 1.5466368797475483\n",
      "parameters: max_depth 10, n_trees 244, number of features per split 3\n",
      "attempt 1\n",
      "attempt 2\n",
      "attempt 2, average error 1.390881003538186\n",
      "parameters: max_depth 16, n_trees 237, number of features per split 6\n",
      "attempt 3\n",
      "attempt 4\n",
      "attempt 5\n",
      "attempt 5, average error 1.3651359519472008\n",
      "parameters: max_depth 12, n_trees 195, number of features per split 16\n",
      "attempt 6\n",
      "attempt 6, average error 1.3573334826502863\n",
      "parameters: max_depth 26, n_trees 159, number of features per split 9\n",
      "attempt 7\n",
      "attempt 8\n",
      "attempt 9\n",
      "attempt 10\n",
      "attempt 11\n",
      "attempt 12\n",
      "attempt 13\n",
      "attempt 14\n",
      "attempt 15\n",
      "attempt 16\n",
      "attempt 17\n",
      "attempt 17, average error 1.3542418696405762\n",
      "parameters: max_depth 17, n_trees 219, number of features per split 9\n",
      "attempt 18\n",
      "attempt 19\n",
      "attempt 19, average error 1.3539234672479161\n",
      "parameters: max_depth 25, n_trees 168, number of features per split 10\n",
      "finished\n",
      "attempt 0, average error 1.2970889908013004\n",
      "parameters: layers 25-17-24, epochs 980, L2 8.592660207062335e-05, learning_rate 0.0002975803318149913\n",
      "attempt 1, average error 1.272197181103165\n",
      "parameters: layers 21-22-11, epochs 695, L2 0.0004876394052327872, learning_rate 0.00015440841049167296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (183) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempt 4, average error 1.2640874623100375\n",
      "parameters: layers 22-17-10, epochs 680, L2 0.00027453992211832156, learning_rate 0.0008431443403465931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (662) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (898) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempt 7, average error 1.2627302273679135\n",
      "parameters: layers 27-30-30, epochs 571, L2 0.0037701118194159713, learning_rate 0.0002118305675106496\n",
      "attempt 8, average error 1.246605076011862\n",
      "parameters: layers 29-26-11, epochs 688, L2 2.8023084192813437e-06, learning_rate 0.0001256440405645265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (640) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (903) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (185) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (622) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (345) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (442) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (313) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "def random_forest():\n",
    "    train_data = pd.read_csv('train_data.csv')\n",
    "    dataComplete = np.array(train_data)\n",
    "    n_folders = 5\n",
    "    predictionsTrainAll = dataComplete[:, 1] / 100000\n",
    "    dataTrainAll = dataComplete[:, 2:]\n",
    "    dates = [int(i.split('T')[0]) for i in dataComplete[:, 0]]\n",
    "    dates = np.asarray(dates)\n",
    "    dataTrainAll = np.hstack((dates[:, np.newaxis], dataTrainAll))\n",
    "    max_depthBest = -1\n",
    "    n_estimatorsBest = -1\n",
    "    max_featuresBest = -1\n",
    "    minCost = float(\"inf\");\n",
    "\n",
    "    for i in range(0, 20):\n",
    "        max_depthCur = random.randint(1, 30)\n",
    "        n_estimatorsCur = random.randint(50, 250)\n",
    "        max_featuresCur = random.uniform(0.1, 1)\n",
    "        curCost = 0;\n",
    "        for j in range(0, n_folders):\n",
    "            selectedRawsTrain = []\n",
    "            selectedRawsTest  = []\n",
    "            for k in range(int(j * dataTrainAll.shape[0] / n_folders), int((j + 1) * dataTrainAll.shape[0] / n_folders)):\n",
    "                selectedRawsTest.append(k)\n",
    "            for k in range(0, int(j * dataTrainAll.shape[0] / n_folders)):\n",
    "                selectedRawsTrain.append(k)\n",
    "            for k in range(int((j + 1) * dataTrainAll.shape[0] / n_folders), dataTrainAll.shape[0]):\n",
    "                selectedRawsTrain.append(k)\n",
    "                \n",
    "            dataTrain        = dataTrainAll[selectedRawsTrain, :]\n",
    "            predictionsTrain = predictionsTrainAll[selectedRawsTrain]\n",
    "            dataTest         = dataTrainAll[selectedRawsTest, :]\n",
    "            predictionsTest  = predictionsTrainAll[selectedRawsTest]           \n",
    "            regr_1 = RandomForestRegressor(max_depth = max_depthCur, n_estimators = n_estimatorsCur, max_features=max_featuresCur, min_samples_split=25)\n",
    "            regr_1.fit(dataTrain, predictionsTrain)\n",
    "            y_1 = regr_1.predict(dataTest)       \n",
    "            terrorList =predictionsTest - y_1\n",
    "            terror = sum(terrorList*terrorList)\n",
    "            curCost += terror\n",
    "\n",
    "        print('attempt {0}'.format(i))\n",
    "\n",
    "        if(curCost < minCost):\n",
    "            print('attempt {0}, average error {1}'.format(i, math.sqrt(curCost / dataTrainAll.shape[0])))\n",
    "            minCost = curCost\n",
    "            max_depthBest = max_depthCur\n",
    "            n_estimatorsBest = n_estimatorsCur\n",
    "            max_featuresBest = max_featuresCur\n",
    "            print('parameters: max_depth {0}, n_trees {1}, number of features per split {2}'.format(max_depthBest, n_estimatorsBest, int(max_featuresBest * dataTrainAll.shape[1])))\n",
    "\n",
    "    print('finished')\n",
    "    regr_1 = RandomForestRegressor(max_depth = max_depthBest, n_estimators = n_estimatorsBest, max_features=max_featuresBest, min_samples_split=25)\n",
    "    regr_1.fit(dataTrainAll, predictionsTrainAll)       \n",
    "    test_data = pd.read_csv('test_data.csv')\n",
    "    dataTestComplete = np.array(test_data);\n",
    "    dataTest = dataTestComplete[:, 1:]\n",
    "    dates = [int(i.split('T')[0]) for i in dataTestComplete[:, 0]]\n",
    "    dates = np.asarray(dates)\n",
    "    dataTest = np.hstack((dates[:, np.newaxis], dataTest))\n",
    "    y_pred = regr_1.predict(dataTest) * 100000\n",
    "    df = pd.DataFrame(y_pred, columns = ['price'])\n",
    "    df.to_csv('results_randomForest.csv')\n",
    "\n",
    "    \n",
    "def neural_networks():\n",
    "\n",
    "    train_data = pd.read_csv('train_data.csv')\n",
    "    dataComplete = np.array(train_data)\n",
    "    n_folders = 5\n",
    "    predictionsTrainAll = dataComplete[:, 1] / 100000\n",
    "    dataTrainAll = dataComplete[:, 2:]\n",
    "    dates = [int(i.split('T')[0]) for i in dataComplete[:, 0]]\n",
    "    dates = np.asarray(dates)\n",
    "    dataTrainAll = np.hstack((dates[:, np.newaxis], dataTrainAll))\n",
    "\n",
    "    tmean = np.mean(dataTrainAll, axis=0)\n",
    "    tstd  = np.std(dataTrainAll, axis=0, dtype=np.float64)\n",
    "    dataTrainAll = (dataTrainAll - tmean) / tstd\n",
    "        \n",
    "    hidden_layer_size1Best = -1\n",
    "    hidden_layer_size2Best = -1\n",
    "    hidden_layer_size3Best = -1\n",
    "    max_iterBest = -1\n",
    "    alphaBest = -1\n",
    "    learning_rateBest = -1\n",
    "    minCost = float(\"inf\");\n",
    "    for i in range(0, 20):\n",
    "\n",
    "        hidden_layer_size1Cur = random.randint(10, 30)\n",
    "        hidden_layer_size2Cur = random.randint(10, 30)\n",
    "        hidden_layer_size3Cur = random.randint(10, 30)\n",
    "        max_iterCur = random.randint(100, 1000)\n",
    "        alphaCur = 10 ** random.uniform(-2, -6)\n",
    "        learning_rateCur = 10 ** random.uniform(-3, -6) \n",
    "        curCost = 0;\n",
    "            \n",
    "            \n",
    "        for j in range(0, n_folders):\n",
    "            selectedRawsTrain = []\n",
    "            selectedRawsTest  = []\n",
    "            for k in range(int(j * dataTrainAll.shape[0] / n_folders), int((j + 1) * dataTrainAll.shape[0] / n_folders)):\n",
    "                selectedRawsTest.append(k)\n",
    "            for k in range(0, int(j * dataTrainAll.shape[0] / n_folders)):\n",
    "                selectedRawsTrain.append(k)\n",
    "            for k in range(int((j + 1) * dataTrainAll.shape[0] / n_folders), dataTrainAll.shape[0]):\n",
    "                selectedRawsTrain.append(k)\n",
    "                \n",
    "            dataTrain        = dataTrainAll[selectedRawsTrain, :]\n",
    "            predictionsTrain = predictionsTrainAll[selectedRawsTrain]\n",
    "            dataTest         = dataTrainAll[selectedRawsTest, :]\n",
    "            predictionsTest  = predictionsTrainAll[selectedRawsTest]\n",
    "\n",
    "            regr_1 = MLPRegressor(hidden_layer_sizes=(hidden_layer_size1Cur, hidden_layer_size2Cur, hidden_layer_size3Cur), batch_size = 100, \\\n",
    "                                  max_iter = max_iterCur, verbose = False, validation_fraction = 0.1, alpha = alphaCur, learning_rate_init = learning_rateCur)#0.00001 * 2 ** talpha)\n",
    "            #regr_1 = MLPRegressor(hidden_layer_sizes=(20, 15, 20), batch_size = 100, max_iter = 500, verbose = False, validation_fraction = 0.1, alpha = 0.0005, learning_rate_init = 0.0005)\n",
    "            regr_1.fit(dataTrain, predictionsTrain)\n",
    "            #regr_1 = RandomForestRegressor(max_depth = max_depthCur, n_estimators = n_estimatorsCur, max_features=max_featuresCur, min_samples_split=25)\n",
    "            #regr_1.fit(dataTrain, predictionsTrain)\n",
    "            y_1 = regr_1.predict(dataTest)       \n",
    "            terrorList =predictionsTest - y_1\n",
    "            terror = sum(terrorList*terrorList)\n",
    "            curCost += terror\n",
    "            #print('attempt {0}: folder {1}'.format(i, j))\n",
    "\n",
    "        if(curCost < minCost):\n",
    "            print('attempt {0}, average error {1}'.format(i, math.sqrt(curCost / dataTrainAll.shape[0])))\n",
    "            minCost = curCost\n",
    "            hidden_layer_size1Best = hidden_layer_size1Cur\n",
    "            hidden_layer_size2Best = hidden_layer_size2Cur\n",
    "            hidden_layer_size3Best = hidden_layer_size3Cur\n",
    "            max_iterBest = max_iterCur\n",
    "            alphaBest = alphaCur\n",
    "            learning_rateBest = learning_rateCur\n",
    "            print('parameters: layers {0}-{1}-{2}, epochs {3}, L2 {4}, learning_rate {5}'.format(hidden_layer_size1Best, hidden_layer_size2Best, \\\n",
    "                  hidden_layer_size3Best, max_iterBest, alphaBest, learning_rateBest))\n",
    "\n",
    "    regr_1 = MLPRegressor(hidden_layer_sizes=(hidden_layer_size1Best, hidden_layer_size2Best, hidden_layer_size3Best), batch_size = 100, \\\n",
    "                          max_iter = max_iterBest, verbose = False, validation_fraction = 0.1, alpha = alphaBest, learning_rate_init = learning_rateBest)\n",
    "    regr_1.fit(dataTrainAll, predictionsTrainAll)       \n",
    "    test_data = pd.read_csv('test_data.csv')\n",
    "    dataTestComplete = np.array(test_data);\n",
    "    dataTest = dataTestComplete[:, 1:]\n",
    "    dates = [int(i.split('T')[0]) for i in dataTestComplete[:, 0]]\n",
    "    dates = np.asarray(dates)\n",
    "    dataTest = np.hstack((dates[:, np.newaxis], dataTest))\n",
    "    dataTest  = (dataTest  - tmean) / tstd\n",
    "    y_pred = regr_1.predict(dataTest) * 100000\n",
    "    df = pd.DataFrame(y_pred, columns = ['price'])\n",
    "    df.to_csv('results_deepLearning.csv')\n",
    "    \n",
    "    \n",
    "random_forest()\n",
    "neural_networks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
